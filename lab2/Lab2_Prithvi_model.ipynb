{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe61ea22",
   "metadata": {},
   "source": [
    "# ğŸ§ª Lab 2: Geospatial AI with the Prithvi Foundation Model\n",
    "\n",
    "**Objective:** Learn to use a powerful, pre-trained geospatial foundation model (`Prithvi-100M-sen1floods11`) to perform a real-world analysis: **flood segmentation**.\n",
    "\n",
    "**Why this is important:** This lab demonstrates how to run state-of-the-art AI models *directly within a notebook* using only the **CPU**. This \"local inference\" workflow is a key skill, allowing you to quickly test models and analyze data without complex, expensive GPU infrastructure.\n",
    "\n",
    "### ğŸ§  What is Prithvi?\n",
    "\n",
    "**Prithvi** (Sanskrit for \"Earth\") is a new class of **Geospatial Foundation Model (GFM)** developed by IBM and NASA. Unlike traditional AI models that are trained on one specific task (like *only* floods or *only* burn scars), Prithvi was pre-trained on a massive, diverse dataset of satellite imagery (from the HLS dataset) from across the entire United States.\n",
    "\n",
    "This \"foundation\" training gives it a deep, generalized understanding of what land, water, vegetation, and urban areas look like in different seasons and conditions. We can then \"fine-tune\" this base model for specific tasks. The model we are using today, `Prithvi-100M-sen1floods11`, has been fine-tuned to be an expert at one thing: **identifying water**.\n",
    "\n",
    "We will run the entire processâ€”from finding data to visualizing the AI's predictionâ€”right here in this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3d4e5",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "  * **Foundation Model (FM):** A large AI model (like Prithvi) pre-trained on vast amounts of general data. This \"foundation\" allows it to be easily adapted to new, specific tasks.\n",
    "  * **Inference:** The process of *using* a trained model to make predictions on new data. This is what we are doing today. (The opposite is *training*).\n",
    "  * **STAC (SpatioTemporal Asset Catalog):** A modern \"search engine\" or API for geospatial data. We use it to find the exact satellite images we need from cloud providers like the Microsoft Planetary Computer.\n",
    "  * **Sentinel-2:** An advanced Earth observation mission providing high-resolution (10m) optical imagery. Its multiple **spectral bands** (seeing beyond just Red, Green, and Blue) are perfect for AI.\n",
    "  * **GeoTIFF:** A standard file format for satellite images. It's a \"geospatial\" TIFF, meaning it contains crucial metadata like coordinates (latitude/longitude) and the map projection.\n",
    "  * **TerraTorch:** An open-source library used to easily load and work with geospatial foundation models like Prithvi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8c7d6",
   "metadata": {},
   "source": [
    "# 1. âš™ï¸ Verify Your Environment\n",
    "\n",
    "This first code cell is a critical check. It **verifies that you are running the correct Jupyter kernel** (`geo-labs-lab2`).\n",
    "\n",
    "**Why we do this:** All the specialized libraries for this lab (like `terratorch`, `pystac_client`, and `mmseg`) have been pre-installed into a specific environment. If you run this notebook with the default kernel, the code in the later steps will fail.\n",
    "\n",
    "* **If you see `âœ… Correct kernel`**, you are all set! Move to the next step.\n",
    "* **If you see `âš ï¸ WARNING`**, please follow the instructions printed in the output to change your kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02e787",
   "metadata": {},
   "source": [
    "If numpy < 2 is installed no need to run\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2\", \"--force-reinstall\", \"--no-deps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if we're in the correct environment\n",
    "current_env = os.path.basename(sys.prefix)\n",
    "expected_env = \"geo-labs-lab2\"\n",
    "\n",
    "if current_env == expected_env:\n",
    "    print(f\"âœ… Correct kernel: {current_env}\")\n",
    "    print(\"All packages are pre-installed via the setup script.\")\n",
    "else:\n",
    "    print(f\"âš ï¸  WARNING: Wrong kernel detected!\")\n",
    "    print(f\"   Current: {current_env}\")\n",
    "    print(f\"   Expected: {expected_env}\")\n",
    "    print()\n",
    "    print(\"Please change your kernel:\")\n",
    "    print(\"   1. Click 'Kernel' â†’ 'Change Kernel'\")\n",
    "    print(\"   2. Select 'Python (geo-labs-lab2)'\")\n",
    "    print(\"   3. Re-run this cell\")\n",
    "    raise RuntimeError(f\"Wrong kernel: {current_env}. Please select '{expected_env}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 2\\. ğŸ“š Import Libraries\n",
    "\n",
    "Now that we've confirmed our environment, this cell will **import all the specific Python libraries** we need. We are loading tools for several key tasks, grouped by their function:\n",
    "\n",
    "  * **AI & Deep Learning:** `torch` (PyTorch) is the core deep learning framework. `mmseg`, `mmcv`, and `terratorch` are helper libraries specifically for loading and running segmentation models like Prithvi.\n",
    "  * **Geospatial Data:** `rasterio`, `geopandas`, and `gdal` are the industry-standard tools for opening, handling, and reprojecting satellite images (GeoTIFFs) and vector data (like our AOI).\n",
    "  * **Data Search:** `pystac_client` and `planetary_computer` allow us to connect to and search the STAC catalog.\n",
    "  * **Visualization:** `leafmap` provides the interactive map for our final result.\n",
    "  * **Standard Utilities:** `requests`, `numpy`, `os`, and `time` are used for downloading files, numerical operations, and timing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import leafmap\n",
    "import numpy as np\n",
    "import torch\n",
    "import rasterio\n",
    "import rasterio.warp\n",
    "import shutil\n",
    "import time\n",
    "import imageio.v2 as imageio\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Visualization libraries\n",
    "from IPython.display import HTML, display\n",
    "from ipyleaflet import Marker, Popup\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# STAC & Geospatial Libraries\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, shape\n",
    "\n",
    "# GDAL\n",
    "from osgeo import gdal\n",
    "\n",
    "import mmcv\n",
    "import mmseg\n",
    "\n",
    "# AI Model Libraries\n",
    "from huggingface_hub import hf_hub_download\n",
    "from terratorch.models import EncoderDecoderFactory\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MMCV version: {mmcv.__version__}\")\n",
    "print(f\"MMSeg version: {mmseg.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5",
   "metadata": {},
   "source": [
    "# 3\\. ğŸ›°ï¸ Find Data with STAC (Sentinel-2)\n",
    "\n",
    "Before we can run our model, we need data. We can't just \"Google\" for satellite images; we need a machine-readable way to find analysis-ready data. This is where **STAC (SpatioTemporal Asset Catalog)** comes in.\n",
    "\n",
    "This code block will:\n",
    "\n",
    "1.  **Define an Area of Interest (AOI):** We've chosen coordinates over **Vicksburg, Mississippi**. This area is a classic example of a complex river system (the Mississippi and Yazoo Rivers) with surrounding floodplains, making it a great test for our model.\n",
    "2.  **Connect to a STAC Catalog:** We'll connect to the **Microsoft Planetary Computer**, a massive, open catalog of geospatial data.\n",
    "3.  **Search for Data:** We will search for a `sentinel-2-l2a` (Level-2A, analysis-ready) image that intersects our AOI, was taken during the Spring 2023 flood season, and has **low cloud cover** (`\"lt\": 30`). Seeing the ground is essential\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7g8h9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Area of Interest - Mississippi River near Vicksburg (inland floods)\n",
    "min_lon, min_lat, max_lon, max_lat = [-91.2, 32.2, -90.8, 32.5]\n",
    "aoi_geometry = box(min_lon, min_lat, max_lon, max_lat)\n",
    "aoi_gdf = gpd.GeoDataFrame(geometry=[aoi_geometry], crs=\"EPSG:4326\")\n",
    "\n",
    "# 2. Connect to Planetary Computer STAC\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "print(\"ğŸ›°ï¸ Connected to Planetary Computer STAC catalog.\")\n",
    "\n",
    "# 3. Search for Sentinel-2 data during flood season\n",
    "time_of_interest = \"2023-04-01/2023-05-31\"  # Spring flood season\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    intersects=aoi_gdf.geometry[0],\n",
    "    datetime=time_of_interest,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 30}}\n",
    ")\n",
    "\n",
    "items = list(search.items())\n",
    "if len(items) == 0:\n",
    "    raise Exception(\"No Sentinel-2 items found. Check your internet connection.\")\n",
    "\n",
    "first_item = items[0]\n",
    "print(f\"âœ… Found {len(items)} items. Selected: {first_item.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7g8h9i0-md-pre",
   "metadata": {},
   "source": [
    "# 4\\. â¬‡ï¸ Download the Data\n",
    "\n",
    "Our STAC search has *found* a matching Sentinel-2 scene. This cell will **download the specific band files** we need.\n",
    "\n",
    "**Why these bands?** A \"true color\" image (like the `rendered_preview` or your phone's camera) only has 3 bands: Red, Green, and Blue. A Sentinel-2 scene is **multispectral**â€”it captures light in many different wavelengths, including those invisible to the human eye.\n",
    "\n",
    "The `Prithvi-100M-sen1floods11` model was specifically trained on **6 bands** to be extra-perceptive:\n",
    "\n",
    "  * `B02` (Blue), `B03` (Green), `B04` (Red) - Visible light\n",
    "  * `B08` (Near-Infrared / NIR) - Key for seeing vegetation health.\n",
    "  * `B11` (SWIR1), `B12` (SWIR2) - **Short-Wave Infrared**. These bands are *excellent* at detecting moisture and distinguishing water from land, which is why they are critical for a flood model.\n",
    "\n",
    "We will also download the `rendered_preview` (a simple JPG) to use as a \"True Color\" background image on our final map for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7g8h9i0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 band names for Prithvi model\n",
    "required_assets = {\n",
    "    \"B02\": \"Blue.tif\",     # Blue (10m)\n",
    "    \"B03\": \"Green.tif\",    # Green (10m)\n",
    "    \"B04\": \"Red.tif\",      # Red (10m)\n",
    "    \"B08\": \"Nir.tif\",      # NIR (10m)\n",
    "    \"B11\": \"Swir1.tif\",    # SWIR1 (20m)\n",
    "    \"B12\": \"Swir2.tif\",    # SWIR2 (20m)\n",
    "}\n",
    "\n",
    "data_dir = \"hls_mississippi_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Clean old data to avoid conflicts with previous runs\n",
    "if os.path.exists(data_dir):\n",
    "    try:\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"â™»ï¸  Refreshing data directory...\")\n",
    "    except PermissionError:\n",
    "        print(f\"âš ï¸  Data directory in use - will overwrite files instead\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Note: {e}\")\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to download files\n",
    "def download_file(url, folder, filename):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"â¬‡ï¸ Downloading {filename}...\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        r.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        print(f\"âœ… {filename} already exists.\")\n",
    "    return filepath\n",
    "\n",
    "# Loop and download all required files\n",
    "band_filepaths = []\n",
    "true_color_path = \"\"\n",
    "for asset_name, filename in required_assets.items():\n",
    "    try:\n",
    "        href = first_item.assets[asset_name].href\n",
    "        fpath = download_file(href, data_dir, filename)\n",
    "        if asset_name == 'browse':\n",
    "            true_color_path = fpath\n",
    "        else:\n",
    "            # Add the 6 model bands to a list\n",
    "            band_filepaths.append(fpath)\n",
    "    except KeyError:\n",
    "        print(f\"âš ï¸ Warning: Asset '{asset_name}' not found in item. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading {asset_name}: {e}\")\n",
    "\n",
    "# Separately download the true color image for visualization\n",
    "true_color_path = \"\"\n",
    "try:\n",
    "    href = first_item.assets[\"rendered_preview\"].href\n",
    "    true_color_path = download_file(href, data_dir, \"True_Color.jpg\")\n",
    "    print(\"âœ“ Downloaded true color preview for visualization\")\n",
    "except:\n",
    "    print(\"âš ï¸ True color preview not available\")\n",
    "\n",
    "print(f\"\\nâœ… Downloaded {len(band_filepaths)} bands for model inference\")\n",
    "print(\"Contents:\", os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7g8h9i0-md-post",
   "metadata": {},
   "source": [
    "Great! Your `hls_mississippi_data` folder should now be populated with the GeoTIFF files for all 6 bands, plus the `True_Color.jpg` preview. We are ready to run the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ba56e",
   "metadata": {},
   "source": [
    "# 5\\. ğŸ§  Load AI Model & Run Inference\n",
    "\n",
    "This is the core of the lab. The following cell performs the entire AI analysis. Here is a breakdown of what it's doing:\n",
    "\n",
    "1.  **Define Helper Functions:**\n",
    "\n",
    "      * `normalize_and_stack`: This is crucial. Our 6 Sentinel-2 bands have different resolutions (10m and 20m). This function will **reproject** all bands to match a single reference grid, **stack** them into one 6-layer file, and **normalize** the pixel values. Normalization (scaling values to a standard range) is a required step to prepare data for an AI model.\n",
    "      * `resize_tensor`: The original satellite image is *huge* (over 10,000x10,000 pixels). Running the model on the full image would be very slow on a CPU. We resize it to `512x512` for *inference*, and then we will scale the *result* back up. This is a common technique to balance speed and accuracy.\n",
    "\n",
    "2.  **Download Model Checkpoint:** Downloads the pre-trained `Prithvi-100M-sen1floods11` model weights (`.pth` file) from the **Hugging Face Hub**. This file contains the \"brain\" of the model with all the learned parameters.\n",
    "\n",
    "3.  **Build Model Architecture:** Uses `TerraTorch`'s `EncoderDecoderFactory` to construct the \"empty\" skeleton of the Prithvi model. We then load the downloaded weights (the \"brain\") into this skeleton. We also explicitly set the device to **`cpu`**.\n",
    "\n",
    "4.  **Prepare Data:** Uses our helper functions to load, stack, normalize, and resize the 6 TIFs into a single `tensor`. A tensor is the primary data structure used by PyTorch, similar to a multi-dimensional array.\n",
    "\n",
    "5.  **Run Inference:** This is the prediction step. We use `torch.no_grad()` and `model.eval()` to tell PyTorch we are in *inference mode* (predicting), not *training mode*. This makes the process much faster and more memory-efficient. The model's output is a `tensor` where each pixel has two values (a \"score\" for \"Not Water\" and a \"score\" for \"Water\").\n",
    "\n",
    "6.  **Save the Result:** We take the class with the highest score (using `torch.argmax`) to create a binary mask (0 = Land, 1 = Water). We then resize this small mask back up to the original image's high resolution and save it as a new GeoTIFF file named `flood_mask.tif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0i1j2k3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print()\n",
    "# --- ASCII Art changed to gray gradient ---\n",
    "print(\"  \\033[97mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\\033[37m  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\\033[90m  â–ˆâ–ˆ\\033[97m â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\\033[37m â–ˆâ–ˆ   â–ˆâ–ˆ\\033[97m â–ˆâ–ˆ    â–ˆâ–ˆ\\033[90m â–ˆâ–ˆ\\033[0m\")\n",
    "print(\"  \\033[97mâ–ˆâ–ˆ   â–ˆâ–ˆ\\033[37m â–ˆâ–ˆ   â–ˆâ–ˆ\\033[90m â–ˆâ–ˆ\\033[97m    â–ˆâ–ˆ   \\033[37m â–ˆâ–ˆ   â–ˆâ–ˆ\\033[97m â–ˆâ–ˆ    â–ˆâ–ˆ\\033[90m â–ˆâ–ˆ\\033[0m\")\n",
    "print(\"  \\033[97mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\\033[37m  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\\033[90m  â–ˆâ–ˆ\\033[97m    â–ˆâ–ˆ   \\033[37m â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\\033[97m â–ˆâ–ˆ    â–ˆâ–ˆ\\033[90m â–ˆâ–ˆ\\033[0m\")\n",
    "print(\"  \\033[97mâ–ˆâ–ˆ\\033[37m      â–ˆâ–ˆ   â–ˆâ–ˆ\\033[90m â–ˆâ–ˆ\\033[97m    â–ˆâ–ˆ   \\033[37m â–ˆâ–ˆ   â–ˆâ–ˆ\\033[97m  â–ˆâ–ˆ  â–ˆâ–ˆ\\033[90m  â–ˆâ–ˆ\\033[0m\")\n",
    "print(\"  \\033[97mâ–ˆâ–ˆ\\033[37m      â–ˆâ–ˆ   â–ˆâ–ˆ\\033[90m â–ˆâ–ˆ\\033[97m    â–ˆâ–ˆ   \\033[37m â–ˆâ–ˆ   â–ˆâ–ˆ\\033[97m   â–ˆâ–ˆâ–ˆâ–ˆ\\033[90m   â–ˆâ–ˆ\\033[0m\")\n",
    "# --- End of change ---\n",
    "print()\n",
    "print(\"  ğŸ›°ï¸  FLOOD DETECTION MODEL INFERENCE\")\n",
    "print(\"  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print()\n",
    "\n",
    "# --- 1. Helper Function to load and normalize data --- \n",
    "def normalize_and_stack(band_files, reference_profile):\n",
    "    bands = []\n",
    "    ref_height = reference_profile['height']\n",
    "    ref_width = reference_profile['width']\n",
    "    \n",
    "    for f in band_files:\n",
    "        with rasterio.open(f) as src:\n",
    "            source_data = src.read(1)\n",
    "            destination = np.zeros((ref_height, ref_width), dtype=np.float32)\n",
    "            \n",
    "            rasterio.warp.reproject(\n",
    "                source=source_data,\n",
    "                destination=destination,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=reference_profile['transform'],\n",
    "                dst_crs=reference_profile['crs'],\n",
    "                resampling=rasterio.warp.Resampling.bilinear\n",
    "            )\n",
    "            bands.append(destination)\n",
    "    \n",
    "    stacked = np.stack(bands, axis=0)\n",
    "    stacked = stacked.astype(np.float32) / 10000.0\n",
    "    stacked = np.clip(stacked, 0.0, 0.3) / 0.3\n",
    "    return torch.from_numpy(stacked).unsqueeze(0)\n",
    "\n",
    "def resize_tensor(tensor, target_size=512):\n",
    "    \"\"\"Resize to manageable size for CPU inference\"\"\"\n",
    "    from torch.nn.functional import interpolate\n",
    "    return interpolate(tensor, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "# --- STAGE 1: Setup --- \n",
    "print(\"[1/5] Setting up inference environment\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"      â†’ device: {device}\")\n",
    "print(f\"      â†’ target resolution: 512x512 (optimized for CPU)\")\n",
    "print(\"      âœ“ Setup complete\")\n",
    "print()\n",
    "\n",
    "# --- STAGE 2: Load Model --- \n",
    "print(\"[2/5] Loading AI model from HuggingFace\")\n",
    "print(\"      â†’ repository: ibm-nasa-geospatial/Prithvi-100M-sen1floods11\")\n",
    "model_repo = \"ibm-nasa-geospatial/Prithvi-100M-sen1floods11\"\n",
    "model_checkpoint = hf_hub_download(\n",
    "    repo_id=model_repo, \n",
    "    filename=\"sen1floods11_Prithvi_100M.pth\"\n",
    ")\n",
    "print(\"      â†’ building model architecture...\")\n",
    "\n",
    "factory = EncoderDecoderFactory()\n",
    "model = factory.build_model(\n",
    "    task=\"segmentation\",\n",
    "    backbone=\"prithvi_vit_100\",\n",
    "    decoder=\"FCNDecoder\",\n",
    "    num_classes=2,\n",
    "    backbone_kwargs={\"in_channels\": 6}\n",
    ").to(device)\n",
    "\n",
    "print(\"      â†’ loading pre-trained weights...\")\n",
    "checkpoint = torch.load(model_checkpoint, map_location=device)\n",
    "state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "print(\"      âœ“ Model loaded and ready\")\n",
    "print()\n",
    "\n",
    "# --- STAGE 3: Prepare Data ---\n",
    "print(\"[3/5] Preparing satellite imagery\")\n",
    "print(f\"      â†’ reading {len(band_filepaths)} spectral bands...\")\n",
    "\n",
    "with rasterio.open(band_filepaths[0]) as ref_src:\n",
    "    ref_profile = {\n",
    "        'width': ref_src.width,\n",
    "        'height': ref_src.height,\n",
    "        'transform': ref_src.transform,\n",
    "        'crs': ref_src.crs,\n",
    "        'dtype': ref_src.dtypes[0]\n",
    "    }\n",
    "\n",
    "print(f\"      â†’ original image size: {ref_profile['height']} x {ref_profile['width']} pixels\")\n",
    "input_tensor = normalize_and_stack(band_filepaths, ref_profile).to(device)\n",
    "print(f\"      â†’ stacked tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "print(\"      â†’ resizing for CPU inference...\")\n",
    "input_tensor = resize_tensor(input_tensor, target_size=512)\n",
    "print(f\"      â†’ resized tensor shape: {input_tensor.shape}\")\n",
    "print(\"      âœ“ Data prepared\")\n",
    "print()\n",
    "\n",
    "# --- STAGE 4: Run Inference ---\n",
    "print(\"[4/5] Running flood detection inference\")\n",
    "print(\"      â†’ processing with Prithvi foundation model...\")\n",
    "print(\"      â†’ estimated time: 2-3 minutes on CPU\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"      â†’ inference completed in {elapsed:.1f} seconds\")\n",
    "\n",
    "# Extract the logits from ModelOutput\n",
    "if hasattr(output, 'logits'):\n",
    "    logits = output.logits\n",
    "elif hasattr(output, 'output'):\n",
    "    logits = output.output\n",
    "else:\n",
    "    logits = output\n",
    "\n",
    "prediction_mask = torch.argmax(logits, dim=1).squeeze(0)\n",
    "print(f\"      â†’ raw prediction shape: {prediction_mask.shape}\")\n",
    "\n",
    "print(\"      â†’ upscaling to original resolution...\")\n",
    "prediction_mask_resized = torch.nn.functional.interpolate(\n",
    "    prediction_mask.unsqueeze(0).unsqueeze(0).float(),\n",
    "    size=(ref_profile['height'], ref_profile['width']),\n",
    "    mode='nearest'\n",
    ").squeeze().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "print(f\"      â†’ final mask shape: {prediction_mask_resized.shape}\")\n",
    "print(\"      âœ“ Inference complete\")\n",
    "print()\n",
    "\n",
    "# --- STAGE 5: Save Results ---\n",
    "print(\"[5/5] Saving flood segmentation mask\")\n",
    "mask_filepath = \"flood_mask.tif\"\n",
    "\n",
    "out_profile = ref_profile.copy()\n",
    "out_profile.update(count=1, dtype='uint8', compress='lzw')\n",
    "\n",
    "with rasterio.open(mask_filepath, 'w', **out_profile) as dst:\n",
    "    dst.write(prediction_mask_resized, 1)\n",
    "\n",
    "print(f\"      â†’ output file: {mask_filepath}\")\n",
    "print(f\"      â†’ file size: {os.path.getsize(mask_filepath) / 1024:.1f} KB\")\n",
    "print(\"      âœ“ Results saved\")\n",
    "print()\n",
    "\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print(\"âœ“ Pipeline complete - Flood mask ready for visualization\")\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d82f7",
   "metadata": {},
   "source": [
    "# 6\\. ğŸ“Š Visualize the Result\n",
    "\n",
    "The analysis is done! We now have two key files ready for visualization:\n",
    "\n",
    "1.  **`True_Color.jpg`**: The real satellite image preview we downloaded (though we won't use it, as `leafmap` provides its own basemaps).\n",
    "2.  **`flood_mask.tif`**: The AI's prediction of where the water is (a file with 0s for land and 1s for water).\n",
    "\n",
    "However, `flood_mask.tif` is a very large file, just like the original satellite images. Loading this directly into a web map can be slow and clunky. In the next step, we will optimize it by converting it to a **Cloud-Optimized GeoTIFF (COG)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print(\"ğŸ”§ OPTIMIZING FLOOD MASK (COG CONVERSION)\")\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print()\n",
    "\n",
    "import subprocess\n",
    "\n",
    "input_tif  = \"flood_mask.tif\"\n",
    "cog_tif    = \"flood_mask_cog.tif\"\n",
    "\n",
    "# --- PART 1: Find the executable ---\n",
    "gdal_translate_exe = shutil.which(\"gdal_translate\")\n",
    "if not gdal_translate_exe:\n",
    "    print(\"Warning: 'gdal_translate' not found in PATH. Building path manually...\")\n",
    "    python_dir = os.path.dirname(sys.executable)\n",
    "    gdal_translate_exe = os.path.join(python_dir, \"gdal_translate\")\n",
    "\n",
    "if not os.path.exists(gdal_translate_exe):\n",
    "    raise FileNotFoundError(f\"Could not find 'gdal_translate' at the expected path: {gdal_translate_exe}\")\n",
    "\n",
    "print(f\"âœ“ Found gdal_translate executable: {gdal_translate_exe}\")\n",
    "\n",
    "\n",
    "# --- PART 2: Build the correct environment for the subprocess ---\n",
    "# Copy the current environment\n",
    "gdal_env = os.environ.copy()\n",
    "\n",
    "# Get the conda environment's base path (e.g., /anaconda/envs/geo-labs-lab2)\n",
    "conda_prefix = sys.prefix\n",
    "\n",
    "# Define the paths to the data directories\n",
    "proj_lib_path = os.path.join(conda_prefix, 'share', 'proj')\n",
    "gdal_data_path = os.path.join(conda_prefix, 'share', 'gdal')\n",
    "\n",
    "# Set the environment variables\n",
    "gdal_env['PROJ_LIB'] = proj_lib_path\n",
    "gdal_env['GDAL_DATA'] = gdal_data_path\n",
    "\n",
    "print(f\"âœ“ Setting PROJ_LIB for subprocess: {proj_lib_path}\")\n",
    "print(f\"âœ“ Setting GDAL_DATA for subprocess: {gdal_data_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n[1/2] Building Cloud-Optimized Geo-TIFF...\")\n",
    "\n",
    "cmd = [\n",
    "    gdal_translate_exe,  # Use the full path\n",
    "    \"-of\", \"COG\",\n",
    "    \"-co\", \"COMPRESS=DEFLATE\",\n",
    "    \"-co\", \"PREDICTOR=2\",\n",
    "    \"-co\", \"LEVEL=6\",\n",
    "    \"-co\", \"NUM_THREADS=ALL_CPUS\",\n",
    "    \"-co\", \"BLOCKSIZE=512\",\n",
    "    input_tif,\n",
    "    cog_tif\n",
    "]\n",
    "\n",
    "# --- PART 3: Run the subprocess with the custom environment ---\n",
    "# Ensure the input file exists before running\n",
    "if not os.path.exists(input_tif):\n",
    "    print(f\"\\nERROR: Input file '{input_tif}' not found. Please create a dummy file to proceed.\")\n",
    "else:\n",
    "    # Pass the custom environment using the 'env' argument\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=gdal_env)\n",
    "    for line in process.stdout:\n",
    "        print(\"   \", line.strip())\n",
    "\n",
    "    process.wait()\n",
    "\n",
    "    # --- PART 4: Proper error checking ---\n",
    "    if process.returncode == 0 and os.path.exists(cog_tif):\n",
    "        print(\"\\n[2/2] âœ“ COG created:\", cog_tif)\n",
    "    else:\n",
    "        print(\"\\n[2/2] âœ— COG creation failed. Check the GDAL errors above.\")\n",
    "\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b76b3",
   "metadata": {},
   "source": [
    "### â˜ï¸ What is a COG?\n",
    "\n",
    "You'll see the cell above just created a `flood_mask_cog.tif`. **COG** stands for **Cloud-Optimized GeoTIFF**.\n",
    "\n",
    "It's a regular GeoTIFF that has been internally reorganized to support \"tiled\" web streaming. Instead of having to download the *entire* file (many megabytes), a web map (like `leafmap`) can intelligently ask for just the small 512x512 \"tiles\" it needs to fill your screen. This makes map visualization *dramatically* faster.\n",
    "\n",
    "Now, let's use `leafmap` to layer this optimized COG on an interactive map and see how our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print(\"ğŸ“Š VISUALIZATION\")\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print()\n",
    "\n",
    "print(\"[1/3] Initializing map...\")\n",
    "center_lat = (min_lat + max_lat) / 2\n",
    "center_lon = (min_lon + max_lon) / 2\n",
    "\n",
    "# Folium backend initialization\n",
    "m = leafmap.Map(center=(center_lat, center_lon), zoom=16, height=\"600px\")\n",
    "\n",
    "print(f\"      â†’ center: ({center_lat:.2f}, {center_lon:.2f})\")\n",
    "print(f\"      â†’ zoom level: 16\")\n",
    "print(\"      âœ“ Map initialized\")\n",
    "print()\n",
    "\n",
    "print(\"[2/3] Adding layers...\")\n",
    "print(\"      â†’ adding study area boundary...\")\n",
    "\n",
    "aoi_style = {\n",
    "    'color': 'red',\n",
    "    'fillColor': 'transparent',\n",
    "    'weight': 3,\n",
    "    'dashArray': '5, 5'\n",
    "}\n",
    "m.add_gdf(aoi_gdf, style=aoi_style, layer_name=\"Study Area (AOI)\")\n",
    "print(\"      âœ“ Red dashed boundary added\")\n",
    "\n",
    "print(\"      â†’ adding optimized flood detection COG...\")\n",
    "print(\"        (fast tiled rendering)\")\n",
    "\n",
    "# Leafmap (Folium) handles local rasters automatically\n",
    "m.add_raster(\n",
    "    \"flood_mask_cog.tif\",\n",
    "    colormap=\"Blues\",\n",
    "    nodata=0,\n",
    "    layer_name=\"AI Flood Detection\",\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "print(\"      âœ“ Flood mask overlay added\")\n",
    "print()\n",
    "\n",
    "print(\"      â†’ adding prediction highlight...\")\n",
    "popup_location = [32.34, -90.9] \n",
    "\n",
    "# [FIX] Pass HTML string directly (Folium does NOT use widgets)\n",
    "popup_html = 'â¬…ï¸ <b>AI Flood Detection</b><br>(Blue Overlay)'\n",
    "\n",
    "m.add_marker(\n",
    "    location=popup_location,\n",
    "    popup=popup_html,\n",
    "    tooltip=\"AI Prediction\"\n",
    ")\n",
    "print(\"      âœ“ Highlight marker added (click pin to see popup)\")\n",
    "print()\n",
    "\n",
    "print(\"[3/3] Finalizing visualization...\")\n",
    "print(\"      â†’ blue shading: detected water/floods\")\n",
    "print(\"      â†’ red box: analysis area\")\n",
    "print(\"      â†’ use layer control (â†—) to toggle layers\")\n",
    "\n",
    "# Add legend (Folium style)\n",
    "m.add_legend(\n",
    "    title=\"Flood Detection\",\n",
    "    labels=[\"Water/Flood (AI Detected)\"],\n",
    "    colors=[\"#084594\"]\n",
    ")\n",
    "print(\"      âœ“ Legend added\")\n",
    "\n",
    "print(\"      âœ“ Visualization ready\")\n",
    "print()\n",
    "\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print(\"âœ“ Map complete - Exporting to HTML...\")\n",
    "print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print()\n",
    "\n",
    "# Export to standalone HTML\n",
    "m.to_html(\"flood_map.html\")\n",
    "print(\"âœ“ Saved to 'flood_map.html'\")\n",
    "\n",
    "# Display\n",
    "m"
   ]
},
  {
   "cell_type": "markdown",
   "id": "b182103f-md-post",
   "metadata": {},
   "source": [
    "### ğŸ—ºï¸ Analyzing the Output\n",
    "\n",
    "Look at the map! The interactive map above displays the final result of our workflow. (You can use the layer control in the top-right corner to toggle layers on and off).\n",
    "\n",
    "  * **OpenStreetMap (Basemap):** This provides the context of roads and city names, like **Vicksburg**.\n",
    "  * **AI Flood Detection (Blue Overlay):** This is our `flood_mask_cog.tif` file. The Prithvi model has generated this layer. The blue areas represent all the pixels that the AI classified as \"Water\" (class 1).\n",
    "  * **Study Area (Red Box):** This is the AOI we defined back in Step 3.\n",
    "\n",
    "As you can see from the provided sample output, the **blue overlay** aligns *extremely* well with the river channels visible in the satellite imagery (and on the basemap). Notice how it's not just a rough blob; the model has captured the precise, complex shape of the **Mississippi River** and the **Yazoo River** to the north. It correctly identified the main channels, smaller tributaries, and even the \"cut-off\" oxbow lakes in the floodplain. This demonstrates the model's high level of accuracy in segmenting water from land, even in a complex riverine environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3f910",
   "metadata": {},
   "source": [
    "## 7. ğŸ”¬ Validating the Result (Ground Truth Check)\n",
    "\n",
    "Our model produced a prediction, but how do we know it's accurate? This final step, **validation**, is one of the most important parts of any AI workflow. We need to compare our result to a \"ground truth\" to confirm its real-world value.\n",
    "\n",
    "We can do this in two simple ways:\n",
    "\n",
    "#### Method 1: Visual Sanity Check (vs. Basemap)\n",
    "Use the layer control (top right) in your interactive map to toggle the \"AI Flood Detection\" layer on and off. Switch the basemap from \"OpenStreetMap\" to \"Satellite\" (often called `Esri.WorldImagery` or similar) to compare satellite-to-satellite.\n",
    "\n",
    "**Observation 1 (Accuracy):** You should see that the blue overlay perfectly aligns with the permanent river channels (the Mississippi and Yazoo) visible on the satellite basemap. This confirms the model isn't \"hallucinating\" and has a very high spatial accuracy.\n",
    "\n",
    "**Observation 2 (Flooding):** Look closely at the areas outside the main channel, especially near the \"Fort of Vicksburg\" and inside the river bends (oxbows). The blue mask extends beyond the main channel into low-lying floodplain areas. This is the model's \"flood\" detection.\n",
    "\n",
    "#### Method 2: Real-World Event Verification\n",
    "The visual check strongly suggests flooding, but was there *actually* a flood in Vicksburg when this image was taken (April-May 2023)? This is the \"ground truth\" check.\n",
    "\n",
    "A quick search for \"Vicksburg MS flooding April 2023\" confirms our data.\n",
    "\n",
    "**Ground Truth:** The National Weather Service (NWS) reported that the Mississippi River at Vicksburg was in **major flood stage** throughout April and early May 2023. The river crested at over 48 feet (well above the 43-foot flood stage), inundating thousands of acres of surrounding low-lying farmland and floodplain areasâ€”exactly what the model detected.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ Conclusion\n",
    "Our workflow is validated. The `Prithvi-100M-sen1floods11` model:\n",
    "\n",
    "*   Correctly identified all permanent water.\n",
    "*   Detected additional surface water in known floodplains.\n",
    "*   This detection matches ground-truth reports of a major, real-world flood event.\n",
    "\n",
    "This confirms that the model is not just guessingâ€”it's accurately segmenting a real, ongoing event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cbb50",
   "metadata": {},
   "source": [
    "# ğŸš€ Exploration (Optional Next Steps)\n",
    "\n",
    "Congratulations on completing the lab! You've successfully run a state-of-the-art Geospatial Foundation Model. If you have extra time, try these challenges to build on what you've learned.\n",
    "\n",
    "-----\n",
    "\n",
    "### 1\\. Change the AOI\n",
    "\n",
    "Go back to **Step 3 (Cell 9)** and change the `min_lon, min_lat, max_lon, max_lat` coordinates.\n",
    "\n",
    "  * **Idea:** Try a large inland lake, like Lake Okeechobee in Florida.\n",
    "  * *Run* all the cells again and see if the model still works.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "# Challenge: Find Lake Okeechobee\n",
    "# Hint: It's around 26.9Â° N, 80.8Â° W\n",
    "min_lon, min_lat, max_lon, max_lat = [____, 26.7, ____, 27.1]\n",
    "```\n",
    "### 2\\. Try a Different Model (Burn Scar Detection)\n",
    "\n",
    "This workflow isn't just for floods â€” you can swap in a **fire burn-scar detection** model.\n",
    "\n",
    "### ğŸ‘‰ How to do it\n",
    "Go to **Step 5 (Cell 14)** and change the model:\n",
    "\n",
    "- Replace the model repo with:  \n",
    "  `ibm-nasa-geospatial/Prithvi-100M-burn-scar`\n",
    "- On the Hugging Face page, find the correct **checkpoint filename** (the `.pth` file)\n",
    "- Pick a new **AOI over a wildfire region** (California, Australia, etc.)\n",
    "- Update the visualization colormap in **Step 6 (Cell 18)**  \n",
    "  from `\"Blues\"` â†’ `\"Reds\"` or `\"OrRd\"` since fire burn scars are reddish/brown\n",
    "\n",
    "### ğŸ”§ Code to Paste (Step 5 modification)\n",
    "\n",
    "```python\n",
    "# Challenge: Modify Step 5 for the Burn Scar Model\n",
    "model_repo = \"ibm-nasa-geospatial/Prithvi-100M-burn-scar\"\n",
    "model_checkpoint = hf_hub_download(\n",
    "    repo_id=model_repo,\n",
    "    filename=\"______.pth\"  # <-- Find the correct filename on Hugging Face\n",
    ")\n",
    "\n",
    "# Modify Step 6 for the Burn Scar Model\n",
    "m.add_raster(\n",
    "    \"flood_mask_cog.tif\", # You should rename this to \"burn_mask_cog.tif\"\n",
    "    colormap=\"____\", # <-- Use a color that makes sense for fire (e.g., \"Reds\")\n",
    "    nodata=0,\n",
    "    layer_name=\"AI Burn Scar Detection\",\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "```\n",
    "### 3. Compare to a Traditional Index (NDWI)\n",
    "\n",
    "In traditional remote sensing, we use simple spectral indices to highlight features.  \n",
    "One classic example is **NDWI â€” Normalized Difference Water Index**.\n",
    "\n",
    "**Formula:**  \n",
    "\\[\n",
    "\\text{NDWI} = \\frac{Green - NIR}{Green + NIR}\n",
    "\\]\n",
    "\n",
    "**How it works:**  \n",
    "- Load **Green.tif** (Sentinel-2 Band 3) and **Nir.tif** (Band 8) using `rasterio`\n",
    "- Perform the NDWI formula\n",
    "- Save the resulting raster to a GeoTIFF\n",
    "- Optionally add it to your Leafmap display in Step 6  \n",
    "- Then compare:  \n",
    "  - Where does NDWI match the AI flood prediction?  \n",
    "  - Where does the AI model detect water that NDWI misses?  \n",
    "  - Where is NDWI noisy or incorrect?\n",
    "\n",
    "---\n",
    "\n",
    "### NDWI Calculation Code (add as a new cell after Step 4)\n",
    "\n",
    "```python\n",
    "# Challenge: Calculate NDWI (add a new cell after Step 4)\n",
    "print(\"Calculating NDWI...\")\n",
    "\n",
    "# 1. Define file paths\n",
    "green_path = os.path.join(data_dir, \"____.tif\")  # <-- Green band filename (e.g., \"Green.tif\")\n",
    "nir_path = os.path.join(data_dir, \"____.tif\")    # <-- NIR band filename (e.g., \"Nir.tif\")\n",
    "ndwi_path = \"ndwi_result.tif\"\n",
    "\n",
    "# 2. Open files\n",
    "with rasterio.open(green_path) as green_src:\n",
    "    green = green_src.read(1).astype(\"float32\")\n",
    "    profile = green_src.profile\n",
    "\n",
    "with rasterio.open(nir_path) as nir_src:\n",
    "    nir = nir_src.read(1).astype(\"float32\")\n",
    "\n",
    "# 3. Calculate NDWI (with a check for division by zero)\n",
    "# Sentinel-2 reflectance is typically scaled by 10,000 â€” float32 makes this safe\n",
    "numerator = green - nir\n",
    "denominator = green + nir\n",
    "\n",
    "# Avoid division-by-zero\n",
    "ndwi = np.where(denominator == 0, 0, numerator / denominator)\n",
    "\n",
    "# 4. Save the result\n",
    "profile.update(dtype=\"float32\", count=1, compress=\"lzw\")\n",
    "\n",
    "with rasterio.open(ndwi_path, \"w\", **profile) as dst:\n",
    "    dst.write(ndwi, 1)\n",
    "\n",
    "print(f\"âœ“ NDWI calculation complete: {ndwi_path}\")\n",
    "\n",
    "# 5. (Optional) Add this new file to your leafmap in Step 6!\n",
    "# m.add_raster(ndwi_path, colormap=\"RdBu\", layer_name=\"NDWI\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo-labs-lab2)",
   "language": "python",
   "name": "geo-labs-lab2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
