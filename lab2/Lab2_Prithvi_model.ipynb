{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe61ea22",
   "metadata": {},
   "source": [
    "# üß™ Lab 2: Geospatial AI with the Prithvi Foundation Model\n",
    "\n",
    "**Objective:** Learn to use a powerful, pre-trained geospatial foundation model (`Prithvi-100M-sen1floods11`) to perform a real-world analysis: **flood segmentation**.\n",
    "\n",
    "**Why this is important:** This lab demonstrates how to run state-of-the-art AI models *directly within a notebook* using only the **CPU**. This \"local inference\" workflow is a key skill, allowing you to quickly test models and analyze data without complex, expensive GPU infrastructure.\n",
    "\n",
    "### üß† What is Prithvi?\n",
    "\n",
    "**Prithvi** (Sanskrit for \"Earth\") is a new class of **Geospatial Foundation Model (GFM)** developed by IBM and NASA. Unlike traditional AI models that are trained on one specific task (like *only* floods or *only* burn scars), Prithvi was pre-trained on a massive, diverse dataset of satellite imagery (from the HLS dataset) from across the entire United States.\n",
    "\n",
    "This \"foundation\" training gives it a deep, generalized understanding of what land, water, vegetation, and urban areas look like in different seasons and conditions. We can then \"fine-tune\" this base model for specific tasks. The model we are using today, `Prithvi-100M-sen1floods11`, has been fine-tuned to be an expert at one thing: **identifying water**.\n",
    "\n",
    "We will run the entire process‚Äîfrom finding data to visualizing the AI's prediction‚Äîright here in this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3d4e5",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "  * **Foundation Model (FM):** A large AI model (like Prithvi) pre-trained on vast amounts of general data. This \"foundation\" allows it to be easily adapted to new, specific tasks.\n",
    "  * **Inference:** The process of *using* a trained model to make predictions on new data. This is what we are doing today. (The opposite is *training*).\n",
    "  * **STAC (SpatioTemporal Asset Catalog):** A modern \"search engine\" or API for geospatial data. We use it to find the exact satellite images we need from cloud providers like the Microsoft Planetary Computer.\n",
    "  * **Sentinel-2:** An advanced Earth observation mission providing high-resolution (10m) optical imagery. Its multiple **spectral bands** (seeing beyond just Red, Green, and Blue) are perfect for AI.\n",
    "  * **GeoTIFF:** A standard file format for satellite images. It's a \"geospatial\" TIFF, meaning it contains crucial metadata like coordinates (latitude/longitude) and the map projection.\n",
    "  * **TerraTorch:** An open-source library used to easily load and work with geospatial foundation models like Prithvi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8c7d6",
   "metadata": {},
   "source": [
    "# 1. ‚öôÔ∏è Verify Your Environment\n",
    "\n",
    "This first code cell is a critical check. It **verifies that you are running the correct Jupyter kernel** (`geo-labs-lab2`).\n",
    "\n",
    "**Why we do this:** All the specialized libraries for this lab (like `terratorch`, `pystac_client`, and `mmseg`) have been pre-installed into a specific environment. If you run this notebook with the default kernel, the code in the later steps will fail.\n",
    "\n",
    "* **If you see `‚úÖ Correct kernel`**, you are all set! Move to the next step.\n",
    "* **If you see `‚ö†Ô∏è WARNING`**, please follow the instructions printed in the output to change your kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02e787",
   "metadata": {},
   "source": [
    "If numpy < 2 is installed no need to run\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2\", \"--force-reinstall\", \"--no-deps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if we're in the correct environment\n",
    "current_env = os.path.basename(sys.prefix)\n",
    "expected_env = \"geo-labs-lab2\"\n",
    "\n",
    "if current_env == expected_env:\n",
    "    print(f\"‚úÖ Correct kernel: {current_env}\")\n",
    "    print(\"All packages are pre-installed via the setup script.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Wrong kernel detected!\")\n",
    "    print(f\"   Current: {current_env}\")\n",
    "    print(f\"   Expected: {expected_env}\")\n",
    "    print()\n",
    "    print(\"Please change your kernel:\")\n",
    "    print(\"   1. Click 'Kernel' ‚Üí 'Change Kernel'\")\n",
    "    print(\"   2. Select 'Python (geo-labs-lab2)'\")\n",
    "    print(\"   3. Re-run this cell\")\n",
    "    raise RuntimeError(f\"Wrong kernel: {current_env}. Please select '{expected_env}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 2\\. üìö Import Libraries\n",
    "\n",
    "Now that we've confirmed our environment, this cell will **import all the specific Python libraries** we need. We are loading tools for several key tasks, grouped by their function:\n",
    "\n",
    "  * **AI & Deep Learning:** `torch` (PyTorch) is the core deep learning framework. `mmseg`, `mmcv`, and `terratorch` are helper libraries specifically for loading and running segmentation models like Prithvi.\n",
    "  * **Geospatial Data:** `rasterio`, `geopandas`, and `gdal` are the industry-standard tools for opening, handling, and reprojecting satellite images (GeoTIFFs) and vector data (like our AOI).\n",
    "  * **Data Search:** `pystac_client` and `planetary_computer` allow us to connect to and search the STAC catalog.\n",
    "  * **Visualization:** `leafmap` provides the interactive map for our final result.\n",
    "  * **Standard Utilities:** `requests`, `numpy`, `os`, and `time` are used for downloading files, numerical operations, and timing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import leafmap\n",
    "import numpy as np\n",
    "import torch\n",
    "import rasterio\n",
    "import rasterio.warp\n",
    "import shutil\n",
    "import time\n",
    "import imageio.v2 as imageio\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Visualization libraries\n",
    "from IPython.display import HTML, display\n",
    "from ipyleaflet import Marker, Popup\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# STAC & Geospatial Libraries\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, shape\n",
    "\n",
    "# GDAL\n",
    "from osgeo import gdal\n",
    "\n",
    "import mmcv\n",
    "import mmseg\n",
    "\n",
    "# AI Model Libraries\n",
    "from huggingface_hub import hf_hub_download\n",
    "from terratorch.models import EncoderDecoderFactory\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MMCV version: {mmcv.__version__}\")\n",
    "print(f\"MMSeg version: {mmseg.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5",
   "metadata": {},
   "source": [
    "# 3\\. üõ∞Ô∏è Find Data with STAC (Sentinel-2)\n",
    "\n",
    "Before we can run our model, we need data. We can't just \"Google\" for satellite images; we need a machine-readable way to find analysis-ready data. This is where **STAC (SpatioTemporal Asset Catalog)** comes in.\n",
    "\n",
    "This code block will:\n",
    "\n",
    "1.  **Define an Area of Interest (AOI):** We've chosen coordinates over **Vicksburg, Mississippi**. This area is a classic example of a complex river system (the Mississippi and Yazoo Rivers) with surrounding floodplains, making it a great test for our model.\n",
    "2.  **Connect to a STAC Catalog:** We'll connect to the **Microsoft Planetary Computer**, a massive, open catalog of geospatial data.\n",
    "3.  **Search for Data:** We will search for a `sentinel-2-l2a` (Level-2A, analysis-ready) image that intersects our AOI, was taken during the Spring 2023 flood season, and has **low cloud cover** (`\"lt\": 30`). Seeing the ground is essential\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7g8h9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Area of Interest - Mississippi River near Vicksburg (inland floods)\n",
    "min_lon, min_lat, max_lon, max_lat = [-91.2, 32.2, -90.8, 32.5]\n",
    "aoi_geometry = box(min_lon, min_lat, max_lon, max_lat)\n",
    "aoi_gdf = gpd.GeoDataFrame(geometry=[aoi_geometry], crs=\"EPSG:4326\")\n",
    "\n",
    "# 2. Connect to Planetary Computer STAC\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "print(\"üõ∞Ô∏è Connected to Planetary Computer STAC catalog.\")\n",
    "\n",
    "# 3. Search for Sentinel-2 data during flood season\n",
    "time_of_interest = \"2023-04-01/2023-05-31\"  # Spring flood season\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    intersects=aoi_gdf.geometry[0],\n",
    "    datetime=time_of_interest,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 30}}\n",
    ")\n",
    "\n",
    "items = list(search.items())\n",
    "if len(items) == 0:\n",
    "    raise Exception(\"No Sentinel-2 items found. Check your internet connection.\")\n",
    "\n",
    "first_item = items[0]\n",
    "print(f\"‚úÖ Found {len(items)} items. Selected: {first_item.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7g8h9i0-md-pre",
   "metadata": {},
   "source": [
    "# 4\\. ‚¨áÔ∏è Download the Data\n",
    "\n",
    "Our STAC search has *found* a matching Sentinel-2 scene. This cell will **download the specific band files** we need.\n",
    "\n",
    "**Why these bands?** A \"true color\" image (like the `rendered_preview` or your phone's camera) only has 3 bands: Red, Green, and Blue. A Sentinel-2 scene is **multispectral**‚Äîit captures light in many different wavelengths, including those invisible to the human eye.\n",
    "\n",
    "The `Prithvi-100M-sen1floods11` model was specifically trained on **6 bands** to be extra-perceptive:\n",
    "\n",
    "  * `B02` (Blue), `B03` (Green), `B04` (Red) - Visible light\n",
    "  * `B08` (Near-Infrared / NIR) - Key for seeing vegetation health.\n",
    "  * `B11` (SWIR1), `B12` (SWIR2) - **Short-Wave Infrared**. These bands are *excellent* at detecting moisture and distinguishing water from land, which is why they are critical for a flood model.\n",
    "\n",
    "We will also download the `rendered_preview` (a simple JPG) to use as a \"True Color\" background image on our final map for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7g8h9i0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 band names for Prithvi model\n",
    "required_assets = {\n",
    "    \"B02\": \"Blue.tif\",     # Blue (10m)\n",
    "    \"B03\": \"Green.tif\",    # Green (10m)\n",
    "    \"B04\": \"Red.tif\",      # Red (10m)\n",
    "    \"B08\": \"Nir.tif\",      # NIR (10m)\n",
    "    \"B11\": \"Swir1.tif\",    # SWIR1 (20m)\n",
    "    \"B12\": \"Swir2.tif\",    # SWIR2 (20m)\n",
    "}\n",
    "\n",
    "data_dir = \"hls_mississippi_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Clean old data to avoid conflicts with previous runs\n",
    "if os.path.exists(data_dir):\n",
    "    try:\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"‚ôªÔ∏è  Refreshing data directory...\")\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è  Data directory in use - will overwrite files instead\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Note: {e}\")\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to download files\n",
    "def download_file(url, folder, filename):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚¨áÔ∏è Downloading {filename}...\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        r.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        print(f\"‚úÖ {filename} already exists.\")\n",
    "    return filepath\n",
    "\n",
    "# Loop and download all required files\n",
    "band_filepaths = []\n",
    "true_color_path = \"\"\n",
    "for asset_name, filename in required_assets.items():\n",
    "    try:\n",
    "        href = first_item.assets[asset_name].href\n",
    "        fpath = download_file(href, data_dir, filename)\n",
    "        if asset_name == 'browse':\n",
    "            true_color_path = fpath\n",
    "        else:\n",
    "            # Add the 6 model bands to a list\n",
    "            band_filepaths.append(fpath)\n",
    "    except KeyError:\n",
    "        print(f\"‚ö†Ô∏è Warning: Asset '{asset_name}' not found in item. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading {asset_name}: {e}\")\n",
    "\n",
    "# Separately download the true color image for visualization\n",
    "true_color_path = \"\"\n",
    "try:\n",
    "    href = first_item.assets[\"rendered_preview\"].href\n",
    "    true_color_path = download_file(href, data_dir, \"True_Color.jpg\")\n",
    "    print(\"‚úì Downloaded true color preview for visualization\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è True color preview not available\")\n",
    "\n",
    "print(f\"\\n‚úÖ Downloaded {len(band_filepaths)} bands for model inference\")\n",
    "print(\"Contents:\", os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7g8h9i0-md-post",
   "metadata": {},
   "source": [
    "Great! Your `hls_mississippi_data` folder should now be populated with the GeoTIFF files for all 6 bands, plus the `True_Color.jpg` preview. We are ready to run the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ba56e",
   "metadata": {},
   "source": [
    "# 5\\. üß† Load AI Model & Run Inference\n",
    "\n",
    "This is the core of the lab. The following cell performs the entire AI analysis. Here is a breakdown of what it's doing:\n",
    "\n",
    "1.  **Define Helper Functions:**\n",
    "\n",
    "      * `normalize_and_stack`: This is crucial. Our 6 Sentinel-2 bands have different resolutions (10m and 20m). This function will **reproject** all bands to match a single reference grid, **stack** them into one 6-layer file, and **normalize** the pixel values. Normalization (scaling values to a standard range) is a required step to prepare data for an AI model.\n",
    "      * `resize_tensor`: The original satellite image is *huge* (over 10,000x10,000 pixels). Running the model on the full image would be very slow on a CPU. We resize it to `512x512` for *inference*, and then we will scale the *result* back up. This is a common technique to balance speed and accuracy.\n",
    "\n",
    "2.  **Download Model Checkpoint:** Downloads the pre-trained `Prithvi-100M-sen1floods11` model weights (`.pth` file) from the **Hugging Face Hub**. This file contains the \"brain\" of the model with all the learned parameters.\n",
    "\n",
    "3.  **Build Model Architecture:** Uses `TerraTorch`'s `EncoderDecoderFactory` to construct the \"empty\" skeleton of the Prithvi model. We then load the downloaded weights (the \"brain\") into this skeleton. We also explicitly set the device to **`cpu`**.\n",
    "\n",
    "4.  **Prepare Data:** Uses our helper functions to load, stack, normalize, and resize the 6 TIFs into a single `tensor`. A tensor is the primary data structure used by PyTorch, similar to a multi-dimensional array.\n",
    "\n",
    "5.  **Run Inference:** This is the prediction step. We use `torch.no_grad()` and `model.eval()` to tell PyTorch we are in *inference mode* (predicting), not *training mode*. This makes the process much faster and more memory-efficient. The model's output is a `tensor` where each pixel has two values (a \"score\" for \"Not Water\" and a \"score\" for \"Water\").\n",
    "\n",
    "6.  **Save the Result:** We take the class with the highest score (using `torch.argmax`) to create a binary mask (0 = Land, 1 = Water). We then resize this small mask back up to the original image's high resolution and save it as a new GeoTIFF file named `flood_mask.tif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0i1j2k3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.warp\n",
    "import os\n",
    "import time\n",
    "from huggingface_hub import hf_hub_download\n",
    "from terratorch.models import EncoderDecoderFactory\n",
    "\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print()\n",
    "# --- ASCII Art changed to gray gradient ---\n",
    "print(\"  \\033[97m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\\033[37m  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\\033[90m  ‚ñà‚ñà\\033[97m ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\\033[37m ‚ñà‚ñà   ‚ñà‚ñà\\033[97m ‚ñà‚ñà    ‚ñà‚ñà\\033[90m ‚ñà‚ñà\\033[0m\")\n",
    "print(\"  \\033[97m‚ñà‚ñà   ‚ñà‚ñà\\033[37m ‚ñà‚ñà   ‚ñà‚ñà\\033[90m ‚ñà‚ñà\\033[97m    ‚ñà‚ñà   \\033[37m ‚ñà‚ñà   ‚ñà‚ñà\\033[97m ‚ñà‚ñà    ‚ñà‚ñà\\033[90m ‚ñà‚ñà\\033[0m\")\n",
    "print(\"  \\033[97m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\\033[37m  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\\033[90m  ‚ñà‚ñà\\033[97m    ‚ñà‚ñà   \\033[37m ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\\033[97m ‚ñà‚ñà    ‚ñà‚ñà\\033[90m ‚ñà‚ñà\\033[0m\")\n",
    "print(\"  \\033[97m‚ñà‚ñà\\033[37m      ‚ñà‚ñà   ‚ñà‚ñà\\033[90m ‚ñà‚ñà\\033[97m    ‚ñà‚ñà   \\033[37m ‚ñà‚ñà   ‚ñà‚ñà\\033[97m  ‚ñà‚ñà  ‚ñà‚ñà\\033[90m  ‚ñà‚ñà\\033[0m\")\n",
    "print(\"  \\033[97m‚ñà‚ñà\\033[37m      ‚ñà‚ñà   ‚ñà‚ñà\\033[90m ‚ñà‚ñà\\033[97m    ‚ñà‚ñà   \\033[37m ‚ñà‚ñà   ‚ñà‚ñà\\033[97m   ‚ñà‚ñà‚ñà‚ñà\\033[90m   ‚ñà‚ñà\\033[0m\")\n",
    "print()\n",
    "print(\"  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print()\n",
    "\n",
    "print(\"  üõ∞Ô∏è  FLOOD DETECTION: SLIDING WINDOW INFERENCE\")\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "\n",
    "# --- 1. Helper Function (renamed to avoid MONAI conflict) ---\n",
    "\n",
    "def predict_sliding_window_batched(model, input_tensor, window_size=512, stride=512, batch_size=4):\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Batched sliding window inference with downsampling support.\n",
    "    - Processes multiple tiles simultaneously (batch_size=4 for E4s CPU)\n",
    "    - Supports stride to reduce tile count\n",
    "    \"\"\"\n",
    "    if len(input_tensor.shape) == 4:\n",
    "        input_tensor = input_tensor.squeeze(0)\n",
    "        \n",
    "    c, h, w = input_tensor.shape\n",
    "    \n",
    "    # Pad to be divisible by window_size\n",
    "    pad_h = (window_size - h % window_size) % window_size\n",
    "    pad_w = (window_size - w % window_size) % window_size\n",
    "    \n",
    "    padded_input = torch.nn.functional.pad(\n",
    "        input_tensor.unsqueeze(0), \n",
    "        (0, pad_w, 0, pad_h), \n",
    "        mode='reflect'\n",
    "    ).squeeze(0)\n",
    "    \n",
    "    pad_h_total, pad_w_total = padded_input.shape[1], padded_input.shape[2]\n",
    "    output_mask = torch.zeros((pad_h_total, pad_w_total), dtype=torch.uint8)\n",
    "    \n",
    "    # Generate all tile coordinates\n",
    "    tiles = []\n",
    "    for y in range(0, pad_h_total - window_size + 1, stride):\n",
    "        for x in range(0, pad_w_total - window_size + 1, stride):\n",
    "            tiles.append((y, x))\n",
    "    \n",
    "    total_tiles = len(tiles)\n",
    "    print(f\"      ‚Üí processing {total_tiles} tiles in batches of {batch_size}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process in batches\n",
    "        for batch_idx in range(0, total_tiles, batch_size):\n",
    "            batch_tiles = tiles[batch_idx:batch_idx + batch_size]\n",
    "            \n",
    "            # Stack batch\n",
    "            batch = torch.stack([\n",
    "                padded_input[:, y:y+window_size, x:x+window_size] \n",
    "                for y, x in batch_tiles\n",
    "            ]).to(device)\n",
    "            \n",
    "            # Run model on batch\n",
    "            out = model(batch)\n",
    "            \n",
    "            # Handle output\n",
    "            if hasattr(out, 'logits'):\n",
    "                logits = out.logits\n",
    "            elif hasattr(out, 'output'):\n",
    "                logits = out.output\n",
    "            else:\n",
    "                logits = out\n",
    "                \n",
    "            preds = torch.argmax(logits, dim=1).cpu()\n",
    "            \n",
    "            # Place predictions\n",
    "            for i, (y, x) in enumerate(batch_tiles):\n",
    "                output_mask[y:y+window_size, x:x+window_size] = preds[i]\n",
    "            \n",
    "            # Progress\n",
    "            if (batch_idx // batch_size) % 5 == 0:\n",
    "                print(f\"      ‚Üí {batch_idx + len(batch_tiles)}/{total_tiles} tiles\", end='\\r')\n",
    "        \n",
    "        print()\n",
    "                \n",
    "    return output_mask[:h, :w].numpy()\n",
    "\n",
    "\n",
    "def normalize_and_stack(band_files, reference_profile):\n",
    "    \"\"\"Reads and stacks bands, reprojects if needed.\"\"\"\n",
    "    bands = []\n",
    "    ref_height = reference_profile['height']\n",
    "    ref_width = reference_profile['width']\n",
    "    \n",
    "    for f in band_files:\n",
    "        with rasterio.open(f) as src:\n",
    "            # Check if reprojection is needed\n",
    "            if (src.transform != reference_profile['transform'] or \n",
    "                src.width != ref_width or \n",
    "                src.height != ref_height):\n",
    "                \n",
    "                # Need to reproject\n",
    "                destination = np.zeros((ref_height, ref_width), dtype=np.float32)\n",
    "                source_data = src.read(1)  # Read the data first\n",
    "                \n",
    "                rasterio.warp.reproject(\n",
    "                    source=source_data,\n",
    "                    destination=destination,\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=reference_profile['transform'],\n",
    "                    dst_crs=reference_profile['crs'],\n",
    "                    resampling=rasterio.warp.Resampling.bilinear\n",
    "                )\n",
    "                bands.append(destination)\n",
    "            else:\n",
    "                # Can use directly\n",
    "                bands.append(src.read(1).astype(np.float32))\n",
    "    \n",
    "    # Stack and normalize\n",
    "    stacked = np.stack(bands, axis=0)\n",
    "    stacked = stacked.astype(np.float32) / 10000.0  # Sentinel-2 scaling\n",
    "    stacked = np.clip(stacked, 0.0, 0.3) / 0.3      # Normalize to [0, 1]\n",
    "    \n",
    "    return torch.from_numpy(stacked)\n",
    "\n",
    "\n",
    "# --- STAGE 1: Setup --- \n",
    "print(\"[1/4] Setting up environment\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"      ‚Üí device: {device}\")\n",
    "print()\n",
    "\n",
    "# --- STAGE 2: Load Model --- \n",
    "print(\"[2/4] Loading Prithvi model\")\n",
    "model_repo = \"ibm-nasa-geospatial/Prithvi-100M-sen1floods11\"\n",
    "model_filename = \"sen1floods11_Prithvi_100M.pth\"\n",
    "\n",
    "print(f\"      ‚Üí downloading from HuggingFace: {model_repo}\")\n",
    "model_checkpoint = hf_hub_download(repo_id=model_repo, filename=model_filename)\n",
    "\n",
    "print(\"      ‚Üí building model architecture...\")\n",
    "factory = EncoderDecoderFactory()\n",
    "model = factory.build_model(\n",
    "    task=\"segmentation\",\n",
    "    backbone=\"prithvi_vit_100\",\n",
    "    decoder=\"FCNDecoder\",\n",
    "    num_classes=2,\n",
    "    backbone_kwargs={\"in_channels\": 6}\n",
    ").to(device)\n",
    "\n",
    "print(\"      ‚Üí loading weights...\")\n",
    "checkpoint = torch.load(model_checkpoint, map_location=device)\n",
    "model.load_state_dict(checkpoint.get('state_dict', checkpoint), strict=False)\n",
    "print(\"      ‚úì Model ready\")\n",
    "print()\n",
    "\n",
    "# --- STAGE 3: Prepare Data (WITH DOWNSAMPLING) ---\n",
    "print(\"[3/4] Reading and downsampling data for faster CPU inference\")\n",
    "\n",
    "# Target size for downsampling (balances speed vs quality)\n",
    "TARGET_SIZE = 2048  # Reduced from ~11000 to 2048 (5x faster)\n",
    "\n",
    "with rasterio.open(band_filepaths[0]) as src:\n",
    "    ref_profile = src.profile\n",
    "\n",
    "original_h = ref_profile['height']\n",
    "original_w = ref_profile['width']\n",
    "print(f\"      ‚Üí original size: {original_h}√ó{original_w} pixels\")\n",
    "\n",
    "# Calculate scale factor\n",
    "scale_factor = TARGET_SIZE / max(original_h, original_w)\n",
    "new_h = int(original_h * scale_factor)\n",
    "new_w = int(original_w * scale_factor)\n",
    "print(f\"      ‚Üí downsampling to: {new_h}√ó{new_w} pixels (~{scale_factor:.2f}x)\")\n",
    "\n",
    "# Update profile for downsampled dimensions\n",
    "downsampled_profile = ref_profile.copy()\n",
    "downsampled_profile.update({\n",
    "    'height': new_h,\n",
    "    'width': new_w,\n",
    "    'transform': rasterio.transform.from_bounds(\n",
    "        *rasterio.transform.array_bounds(original_h, original_w, ref_profile['transform']),\n",
    "        new_w, new_h\n",
    "    )\n",
    "})\n",
    "\n",
    "full_tensor = normalize_and_stack(band_filepaths, downsampled_profile)\n",
    "print(f\"      ‚Üí final tensor shape: {full_tensor.shape}\")\n",
    "print(\"      ‚úì Data loaded and downsampled\")\n",
    "print()\n",
    "\n",
    "# --- STAGE 4: Inference (BATCHED) ---\n",
    "print(\"[4/4] Running OPTIMIZED batched sliding window inference\")\n",
    "print(\"      ‚è±Ô∏è  Estimated time: 3-4 minutes (optimized for CPU)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use batched inference (4 tiles at once on E4s)\n",
    "downsampled_mask = predict_sliding_window_batched(\n",
    "    model, \n",
    "    full_tensor, \n",
    "    window_size=512, \n",
    "    stride=512,\n",
    "    batch_size=4  # Process 4 tiles simultaneously\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "print(f\"      ‚úì Inference complete in {minutes}m {seconds}s\")\n",
    "\n",
    "# Upscale mask back to original resolution\n",
    "print(f\"\\n      ‚Üí upscaling from {downsampled_mask.shape} to ({original_h}, {original_w})\")\n",
    "final_mask = np.round(\n",
    "    resize(downsampled_mask, (original_h, original_w), order=0, preserve_range=True, anti_aliasing=False)\n",
    ").astype(np.uint8)\n",
    "\n",
    "# Check prediction statistics\n",
    "unique, counts = np.unique(final_mask, return_counts=True)\n",
    "print(f\"\\n      üìä Prediction Statistics:\")\n",
    "for cls, count in zip(unique, counts):\n",
    "    percentage = (count / final_mask.size) * 100\n",
    "    class_name = \"Land\" if cls == 0 else \"Water\"\n",
    "    print(f\"         Class {cls} ({class_name}): {count:,} pixels ({percentage:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Save Result\n",
    "mask_filepath = \"flood_mask.tif\"\n",
    "out_profile = ref_profile.copy()\n",
    "out_profile.update(count=1, dtype='uint8', compress='lzw')\n",
    "\n",
    "with rasterio.open(mask_filepath, 'w', **out_profile) as dst:\n",
    "    dst.write(final_mask, 1)\n",
    "\n",
    "print(f\"      ‚Üí saved to: {mask_filepath}\")\n",
    "print(f\"      ‚Üí file size: {os.path.getsize(mask_filepath) / (1024*1024):.1f} MB\")\n",
    "print()\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(\"‚úÖ Pipeline complete - ready for COG conversion\")\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization check\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "im = ax.imshow(final_mask, cmap='Blues', vmin=0, vmax=1)\n",
    "ax.set_title('Flood Detection Result (0=Land, 1=Water)')\n",
    "plt.colorbar(im, ax=ax, label='Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Shape: {final_mask.shape}\")\n",
    "print(f\"Unique values: {np.unique(final_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d82f7",
   "metadata": {},
   "source": [
    "# 6\\. üìä Visualize the Result\n",
    "\n",
    "The analysis is done! We now have two key files ready for visualization:\n",
    "\n",
    "1.  **`True_Color.jpg`**: The real satellite image preview we downloaded (though we won't use it, as `leafmap` provides its own basemaps).\n",
    "2.  **`flood_mask.tif`**: The AI's prediction of where the water is (a file with 0s for land and 1s for water).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap.foliumap as leafmap\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import folium\n",
    "from folium.raster_layers import ImageOverlay\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "print(\"üìä VISUALIZATION (CLIPPED TO AOI)\")\n",
    "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "\n",
    "# --- 1. Helper Function: Clip Raster to AOI ---\n",
    "def clip_raster_to_aoi(src_tif, aoi_gdf, out_tif):\n",
    "    \"\"\"Crops the raster to the AOI boundary.\"\"\"\n",
    "    with rasterio.open(src_tif) as src:\n",
    "        # 1. Project AOI to match the Raster's CRS (e.g., UTM)\n",
    "        aoi_reprojected = aoi_gdf.to_crs(src.crs)\n",
    "        \n",
    "        # 2. Crop the image\n",
    "        # shapes expects a list of GeoJSON-like geometries\n",
    "        out_image, out_transform = mask(src, aoi_reprojected.geometry, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "        # 3. Update metadata (height, width, transform)\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform\n",
    "        })\n",
    "\n",
    "        # 4. Save clipped file\n",
    "        with rasterio.open(out_tif, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "            \n",
    "    return out_tif\n",
    "\n",
    "# --- 2. Helper Function: Create Overlay (Reproject & Color) ---\n",
    "def create_flood_overlay(tif_path, png_path, downscale_factor=0.2):\n",
    "    if not os.path.exists(tif_path):\n",
    "        return None, None\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        # Destination CRS: Web Mercator/LatLon\n",
    "        dst_crs = 'EPSG:4326'\n",
    "\n",
    "        # Calculate dimensions\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "\n",
    "        dst_width = int(width * downscale_factor)\n",
    "        dst_height = int(height * downscale_factor)\n",
    "\n",
    "        transform, _, _ = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height,\n",
    "            *src.bounds, dst_width=dst_width, dst_height=dst_height\n",
    "        )\n",
    "\n",
    "        destination = np.zeros((dst_height, dst_width), dtype=np.uint8)\n",
    "\n",
    "        reproject(\n",
    "            source=rasterio.band(src, 1),\n",
    "            destination=destination,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=transform,\n",
    "            dst_crs=dst_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "\n",
    "        # Create RGBA Image\n",
    "        rgba = np.zeros((dst_height, dst_width, 4), dtype=np.uint8)\n",
    "        water_mask = (destination == 1)\n",
    "        # Dodger Blue with opacity\n",
    "        rgba[water_mask] = [30, 144, 255, 180]\n",
    "\n",
    "        img = Image.fromarray(rgba)\n",
    "        img.save(png_path)\n",
    "\n",
    "        b = rasterio.transform.array_bounds(dst_height, dst_width, transform)\n",
    "        # Folium bounds: [[lat_min, lon_min], [lat_max, lon_max]]\n",
    "        bounds = [[b[1], b[0]], [b[3], b[2]]]\n",
    "\n",
    "        return png_path, bounds\n",
    "\n",
    "# --- 3. Main Workflow ---\n",
    "print(\"[1/3] finding and processing data...\")\n",
    "\n",
    "# Find input file\n",
    "possible_files = [\"flood_mask (1).tif\", \"flood_mask.tif\"]\n",
    "input_tif = next((f for f in possible_files if os.path.exists(f)), None)\n",
    "\n",
    "if not input_tif:\n",
    "    # Fallback search\n",
    "    tiffs = glob.glob(\"*.tif\")\n",
    "    if tiffs: input_tif = tiffs[0]\n",
    "\n",
    "if input_tif:\n",
    "    print(f\"      ‚Üí Input: {input_tif}\")\n",
    "    \n",
    "    # NEW STEP: Clip to AOI if available\n",
    "    processing_tif = input_tif\n",
    "    if 'aoi_gdf' in locals():\n",
    "        print(\"      ‚Üí Clipping raster to AOI boundary...\")\n",
    "        clipped_tif = \"flood_mask_clipped.tif\"\n",
    "        try:\n",
    "            processing_tif = clip_raster_to_aoi(input_tif, aoi_gdf, clipped_tif)\n",
    "            print(\"      ‚úì Clip successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è Clip failed ({e}), using full image instead.\")\n",
    "    \n",
    "    # Generate Overlay\n",
    "    overlay_png = \"flood_overlay_final.png\"\n",
    "    img_path, img_bounds = create_flood_overlay(processing_tif, overlay_png)\n",
    "\n",
    "    if img_path:\n",
    "        # --- 4. Initialize Map ---\n",
    "        print(\"[2/3] Initializing map...\")\n",
    "        \n",
    "        # Center on the AOI\n",
    "        if 'aoi_gdf' in locals():\n",
    "            aoi_center = aoi_gdf.geometry.iloc[0].centroid\n",
    "            center_lat, center_lon = aoi_center.y, aoi_center.x\n",
    "        else:\n",
    "            center_lat = (img_bounds[0][0] + img_bounds[1][0]) / 2\n",
    "            center_lon = (img_bounds[0][1] + img_bounds[1][1]) / 2\n",
    "\n",
    "        m = leafmap.Map(center=(center_lat, center_lon), zoom=13, height=\"600px\")\n",
    "        m.add_basemap(\"OpenStreetMap\")\n",
    "        m.add_basemap(\"Esri.WorldImagery\")\n",
    "\n",
    "        # --- 5. Add Overlay ---\n",
    "        print(\"[3/3] Adding overlay...\")\n",
    "        \n",
    "        overlay = ImageOverlay(\n",
    "            name=\"AI Flood Prediction\",\n",
    "            image=overlay_png,\n",
    "            bounds=img_bounds,\n",
    "            opacity=0.8,\n",
    "            interactive=True,\n",
    "            cross_origin=False,\n",
    "            zindex=1\n",
    "        )\n",
    "        overlay.add_to(m)\n",
    "\n",
    "        if 'aoi_gdf' in locals():\n",
    "            folium.GeoJson(\n",
    "                data=aoi_gdf,\n",
    "                name=\"Study Area (AOI)\",\n",
    "                style_function=lambda x: {'color': 'red', 'fillColor': 'transparent', 'weight': 2, 'dashArray': '5, 5'}\n",
    "            ).add_to(m)\n",
    "\n",
    "        folium.LayerControl(collapsed=False).add_to(m)\n",
    "        print(\"      ‚úì Visualization ready.\")\n",
    "    else:\n",
    "        print(\"‚ùå Error processing overlay.\")\n",
    "else:\n",
    "    print(\"‚ùå Input TIF file not found.\")\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182103f-md-post",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Analyzing the Output\n",
    "\n",
    "Look at the map! The interactive map above displays the final result of our workflow. (You can use the layer control in the top-right corner to toggle layers on and off).\n",
    "\n",
    "  * **OpenStreetMap (Basemap):** This provides the context of roads and city names, like **Vicksburg**.\n",
    "  * **AI Flood Detection (Blue Overlay):** This is our `flood_mask_cog.tif` file. The Prithvi model has generated this layer. The blue areas represent all the pixels that the AI classified as \"Water\" (class 1).\n",
    "  * **Study Area (Red Box):** This is the AOI we defined back in Step 3.\n",
    "\n",
    "As you can see from the provided sample output, the **blue overlay** aligns *extremely* well with the river channels visible in the satellite imagery (and on the basemap). Notice how it's not just a rough blob; the model has captured the precise, complex shape of the **Mississippi River** and the **Yazoo River** to the north. It correctly identified the main channels, smaller tributaries, and even the \"cut-off\" oxbow lakes in the floodplain. This demonstrates the model's high level of accuracy in segmenting water from land, even in a complex riverine environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3f910",
   "metadata": {},
   "source": [
    "## 7. üî¨ Validating the Result (Ground Truth Check)\n",
    "\n",
    "Our model produced a prediction, but how do we know it's accurate? This final step, **validation**, is one of the most important parts of any AI workflow. We need to compare our result to a \"ground truth\" to confirm its real-world value.\n",
    "\n",
    "We can do this in two simple ways:\n",
    "\n",
    "#### Method 1: Visual Sanity Check (vs. Basemap)\n",
    "Use the layer control (top right) in your interactive map to toggle the \"AI Flood Detection\" layer on and off. Switch the basemap from \"OpenStreetMap\" to \"Satellite\" (often called `Esri.WorldImagery` or similar) to compare satellite-to-satellite.\n",
    "\n",
    "**Observation 1 (Accuracy):** You should see that the blue overlay perfectly aligns with the permanent river channels (the Mississippi and Yazoo) visible on the satellite basemap. This confirms the model isn't \"hallucinating\" and has a very high spatial accuracy.\n",
    "\n",
    "**Observation 2 (Flooding):** Look closely at the areas outside the main channel, especially near the \"Fort of Vicksburg\" and inside the river bends (oxbows). The blue mask extends beyond the main channel into low-lying floodplain areas. This is the model's \"flood\" detection.\n",
    "\n",
    "#### Method 2: Real-World Event Verification\n",
    "The visual check strongly suggests flooding, but was there *actually* a flood in Vicksburg when this image was taken (April-May 2023)? This is the \"ground truth\" check.\n",
    "\n",
    "A quick search for \"Vicksburg MS flooding April 2023\" confirms our data.\n",
    "\n",
    "**Ground Truth:** The National Weather Service (NWS) reported that the Mississippi River at Vicksburg was in **major flood stage** throughout April and early May 2023. The river crested at over 48 feet (well above the 43-foot flood stage), inundating thousands of acres of surrounding low-lying farmland and floodplain areas‚Äîexactly what the model detected.\n",
    "\n",
    "---\n",
    "\n",
    "### üèÅ Conclusion\n",
    "Our workflow is validated. The `Prithvi-100M-sen1floods11` model:\n",
    "\n",
    "*   Correctly identified all permanent water.\n",
    "*   Detected additional surface water in known floodplains.\n",
    "*   This detection matches ground-truth reports of a major, real-world flood event.\n",
    "\n",
    "This confirms that the model is not just guessing‚Äîit's accurately segmenting a real, ongoing event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cbb50",
   "metadata": {},
   "source": [
    "# üöÄ Exploration (Optional Next Steps)\n",
    "\n",
    "Congratulations on completing the lab! You've successfully run a state-of-the-art Geospatial Foundation Model. If you have extra time, try these challenges to build on what you've learned.\n",
    "\n",
    "-----\n",
    "\n",
    "### 1\\. Change the AOI\n",
    "\n",
    "Go back to **Step 3 (Cell 9)** and change the `min_lon, min_lat, max_lon, max_lat` coordinates.\n",
    "\n",
    "  * **Idea:** Try a large inland lake, like Lake Okeechobee in Florida.\n",
    "  * *Run* all the cells again and see if the model still works.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "# Challenge: Find Lake Okeechobee\n",
    "# Hint: It's around 26.9¬∞ N, 80.8¬∞ W\n",
    "min_lon, min_lat, max_lon, max_lat = [____, 26.7, ____, 27.1]\n",
    "```\n",
    "### 2\\. Try a Different Model (Burn Scar Detection)\n",
    "\n",
    "This workflow isn't just for floods ‚Äî you can swap in a **fire burn-scar detection** model.\n",
    "\n",
    "### üëâ How to do it\n",
    "Go to **Step 5 (Cell 14)** and change the model:\n",
    "\n",
    "- Replace the model repo with:  \n",
    "  `ibm-nasa-geospatial/Prithvi-100M-burn-scar`\n",
    "- On the Hugging Face page, find the correct **checkpoint filename** (the `.pth` file)\n",
    "- Pick a new **AOI over a wildfire region** (California, Australia, etc.)\n",
    "- Update the visualization colormap in **Step 6 (Cell 18)**  \n",
    "  from `\"Blues\"` ‚Üí `\"Reds\"` or `\"OrRd\"` since fire burn scars are reddish/brown\n",
    "\n",
    "### üîß Code to Paste (Step 5 modification)\n",
    "\n",
    "```python\n",
    "# Challenge: Modify Step 5 for the Burn Scar Model\n",
    "model_repo = \"ibm-nasa-geospatial/Prithvi-100M-burn-scar\"\n",
    "model_checkpoint = hf_hub_download(\n",
    "    repo_id=model_repo,\n",
    "    filename=\"______.pth\"  # <-- Find the correct filename on Hugging Face\n",
    ")\n",
    "\n",
    "# Modify Step 6 for the Burn Scar Model\n",
    "m.add_raster(\n",
    "    \"flood_mask_cog.tif\", # You should rename this to \"burn_mask_cog.tif\"\n",
    "    colormap=\"____\", # <-- Use a color that makes sense for fire (e.g., \"Reds\")\n",
    "    nodata=0,\n",
    "    layer_name=\"AI Burn Scar Detection\",\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "```\n",
    "### 3. Compare to a Traditional Index (NDWI)\n",
    "\n",
    "In traditional remote sensing, we use simple spectral indices to highlight features.  \n",
    "One classic example is **NDWI ‚Äî Normalized Difference Water Index**.\n",
    "\n",
    "**Formula:**  \n",
    "\\[\n",
    "\\text{NDWI} = \\frac{Green - NIR}{Green + NIR}\n",
    "\\]\n",
    "\n",
    "**How it works:**  \n",
    "- Load **Green.tif** (Sentinel-2 Band 3) and **Nir.tif** (Band 8) using `rasterio`\n",
    "- Perform the NDWI formula\n",
    "- Save the resulting raster to a GeoTIFF\n",
    "- Optionally add it to your Leafmap display in Step 6  \n",
    "- Then compare:  \n",
    "  - Where does NDWI match the AI flood prediction?  \n",
    "  - Where does the AI model detect water that NDWI misses?  \n",
    "  - Where is NDWI noisy or incorrect?\n",
    "\n",
    "---\n",
    "\n",
    "### NDWI Calculation Code (add as a new cell after Step 4)\n",
    "\n",
    "```python\n",
    "# Challenge: Calculate NDWI (add a new cell after Step 4)\n",
    "print(\"Calculating NDWI...\")\n",
    "\n",
    "# 1. Define file paths\n",
    "green_path = os.path.join(data_dir, \"____.tif\")  # <-- Green band filename (e.g., \"Green.tif\")\n",
    "nir_path = os.path.join(data_dir, \"____.tif\")    # <-- NIR band filename (e.g., \"Nir.tif\")\n",
    "ndwi_path = \"ndwi_result.tif\"\n",
    "\n",
    "# 2. Open files\n",
    "with rasterio.open(green_path) as green_src:\n",
    "    green = green_src.read(1).astype(\"float32\")\n",
    "    profile = green_src.profile\n",
    "\n",
    "with rasterio.open(nir_path) as nir_src:\n",
    "    nir = nir_src.read(1).astype(\"float32\")\n",
    "\n",
    "# 3. Calculate NDWI (with a check for division by zero)\n",
    "# Sentinel-2 reflectance is typically scaled by 10,000 ‚Äî float32 makes this safe\n",
    "numerator = green - nir\n",
    "denominator = green + nir\n",
    "\n",
    "# Avoid division-by-zero\n",
    "ndwi = np.where(denominator == 0, 0, numerator / denominator)\n",
    "\n",
    "# 4. Save the result\n",
    "profile.update(dtype=\"float32\", count=1, compress=\"lzw\")\n",
    "\n",
    "with rasterio.open(ndwi_path, \"w\", **profile) as dst:\n",
    "    dst.write(ndwi, 1)\n",
    "\n",
    "print(f\"‚úì NDWI calculation complete: {ndwi_path}\")\n",
    "\n",
    "# 5. (Optional) Add this new file to your leafmap in Step 6!\n",
    "# m.add_raster(ndwi_path, colormap=\"RdBu\", layer_name=\"NDWI\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo-labs-lab2)",
   "language": "python",
   "name": "geo-labs-lab2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
